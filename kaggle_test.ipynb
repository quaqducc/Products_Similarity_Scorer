{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Test: Product Similarity (Multi-agent + Judge)\n",
    "\n",
    "Notebook nÃ y cháº¡y pipeline má»›i:\n",
    "- Analyzer (prompt + few-shot) giá»¯ nguyÃªn tá»« `product_similarity`.\n",
    "- Multi-agent theo tiÃªu chÃ­ (Nature, Intended Purpose, Channel of trade, ...).\n",
    "- Judge gá»™p Ä‘iá»ƒm cÃ¡c tiÃªu chÃ­ Ä‘á»ƒ ra Overall Similarity.\n",
    "\n",
    "Tá»± Ä‘á»™ng phÃ¡t hiá»‡n repo/data tá»« `/kaggle/input`, cÃ³ thá»ƒ cháº¡y:\n",
    "- KhÃ´ng model (chá»‰ build prompt) hoáº·c dÃ¹ng HF local/Chat API.\n",
    "- Cháº¡y Ä‘Æ¡n láº» má»™t case hoáº·c batch tá»« CSV (`data/75_samples.csv` hoáº·c `data/100_samples.csv`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T14:16:09.584148Z",
     "iopub.status.busy": "2025-10-15T14:16:09.583921Z",
     "iopub.status.idle": "2025-10-15T14:16:09.705440Z",
     "shell.execute_reply": "2025-10-15T14:16:09.704698Z",
     "shell.execute_reply.started": "2025-10-15T14:16:09.584130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Paths: adapt names to your uploaded Kaggle Datasets\n",
    "import os, sys, json, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# CHANGE THESE if your dataset names are different\n",
    "REPO_DS_NAME = \"product-similarity-scorer\"   # dataset containing the repo (code files)\n",
    "DATA_DS_NAME = \"products-similarity-scorer-data\"   # dataset containing data/ and/or data_nice_cls/\n",
    "\n",
    "# Kaggle input base\n",
    "KAGGLE_INPUT = Path(\"/kaggle/input\")\n",
    "\n",
    "# Locate mount points\n",
    "repo_root = None\n",
    "for p in KAGGLE_INPUT.glob(f\"{REPO_DS_NAME}*\"):\n",
    "    if (p / \"product_similarity\" / \"prompt.py\").exists():\n",
    "        repo_root = p\n",
    "        break\n",
    "\n",
    "if repo_root is None:\n",
    "    raise RuntimeError(\"KhÃ´ng tÃ¬m tháº¥y repo dataset. Äáº£m báº£o báº¡n Ä‘Ã£ thÃªm Input Dataset cho repo.\")\n",
    "\n",
    "# Add repo to sys.path\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Prepare working directory in /kaggle/working\n",
    "work_dir = Path(\"/kaggle/working/product_similarity_work\")\n",
    "work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy repo files into working dir so we can run/modify locally if needed\n",
    "shutil.copytree(repo_root, work_dir / \"repo\", dirs_exist_ok=True)\n",
    "code_root = work_dir / \"repo\"\n",
    "print(\"Repo root:\", code_root)\n",
    "\n",
    "# Locate data dataset (optional if you only want to run no-model without data)\n",
    "data_root = Path(KAGGLE_INPUT / DATA_DS_NAME)\n",
    "\n",
    "print(\"Data root:\", data_root)\n",
    "\n",
    "# Link/copy data into working repo structure\n",
    "(target_data := code_root / \"data\").mkdir(parents=True, exist_ok=True)\n",
    "# (target_cls := code_root / \"data_nice_cls\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if data_root is not None:\n",
    "    shutil.copytree(data_root, target_data, dirs_exist_ok=True)\n",
    "    # print(\"-\" * 20)\n",
    "    # print(f\"ðŸ“ CÃ¡c file trong thÆ° má»¥c Ä‘Ã­ch '{target_data}':\")\n",
    "    # for root, dirs, files in os.walk(target_data):\n",
    "    #     for name in files:\n",
    "    #         # Táº¡o Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ vÃ  in ra\n",
    "    #         file_path = os.path.join(root, name)\n",
    "    #         print(file_path)\n",
    "    # print(\"-\" * 20)\n",
    "        \n",
    "    # if (data_root / \"data_nice_cls\").exists():\n",
    "    #     shutil.copytree(data_root / \"data_nice_cls\", target_cls, dirs_exist_ok=True)\n",
    "\n",
    "print(\"Prepared data at:\", target_data)\n",
    "\n",
    "# Verify SPSC availability inside the working repo\n",
    "spsc_path = code_root / \"spsc_data\" / \"spsc_data\" / \"spsc_tree.json\"\n",
    "print(\"SPSC tree:\", spsc_path, \"exists=\", spsc_path.exists())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T14:16:09.706715Z",
     "iopub.status.busy": "2025-10-15T14:16:09.706428Z",
     "iopub.status.idle": "2025-10-15T14:16:09.719068Z",
     "shell.execute_reply": "2025-10-15T14:16:09.718325Z",
     "shell.execute_reply.started": "2025-10-15T14:16:09.706688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ensure importable package\n",
    "import importlib\n",
    "pkg_path = code_root / \"product_similarity\"\n",
    "assert (pkg_path / \"__init__.py\").exists(), \"Missing package files in repo dataset!\"\n",
    "\n",
    "# Put working repo to sys.path first\n",
    "import sys\n",
    "sys.path.insert(0, str(code_root))\n",
    "\n",
    "product_similarity = importlib.import_module(\"product_similarity\")\n",
    "print(\"Loaded product_similarity version:\", getattr(product_similarity, \"__version__\", \"unknown\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T14:16:09.720172Z",
     "iopub.status.busy": "2025-10-15T14:16:09.719829Z",
     "iopub.status.idle": "2025-10-15T14:16:09.725899Z",
     "shell.execute_reply": "2025-10-15T14:16:09.725093Z",
     "shell.execute_reply.started": "2025-10-15T14:16:09.720155Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Build nice_chunks.json if missing\n",
    "from pathlib import Path\n",
    "\n",
    "nice_path = code_root / \"data\" / \"nice_chunks.json\"\n",
    "if not nice_path.exists():\n",
    "    print(\"nice_chunks.json missing -> attempt to build from data_nice_cls\")\n",
    "    tools_script = code_root / \"tools\" / \"merge_nice_cls.py\"\n",
    "    if not tools_script.exists():\n",
    "        raise RuntimeError(\"merge_nice_cls.py not found in repo/tools\")\n",
    "    import runpy\n",
    "    runpy.run_path(str(tools_script))\n",
    "else:\n",
    "    print(\"Found:\", nice_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T14:16:09.727281Z",
     "iopub.status.busy": "2025-10-15T14:16:09.726926Z",
     "iopub.status.idle": "2025-10-15T14:16:09.736428Z",
     "shell.execute_reply": "2025-10-15T14:16:09.735714Z",
     "shell.execute_reply.started": "2025-10-15T14:16:09.727263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optional: set NV_API_KEY here if not using Kaggle Secrets\n",
    "import os\n",
    "if \"NV_API_KEY\" not in os.environ:\n",
    "\t# os.environ[\"NV_API_KEY\"] = \"YOUR_KEY_HERE\"  # uncomment to set manually\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T14:16:09.737693Z",
     "iopub.status.busy": "2025-10-15T14:16:09.737416Z",
     "iopub.status.idle": "2025-10-15T14:16:09.747135Z",
     "shell.execute_reply": "2025-10-15T14:16:09.746337Z",
     "shell.execute_reply.started": "2025-10-15T14:16:09.737670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Toggles\n",
    "USE_ANALYZER_MODEL = None  # e.g., \"google/flan-t5-base\" or None to skip analyzer generation\n",
    "AGENT_MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"  # multi-agent default model\n",
    "DEVICE = -1  # -1 CPU, 0 GPU\n",
    "MAX_NEW_TOKENS = 256\n",
    "\n",
    "# Use OpenAI-compatible Chat API (e.g., NVIDIA) (applies to both analyzer and agents if provided)\n",
    "USE_CHAT_API = False\n",
    "CHAT_API_BASE_URL = \"https://integrate.api.nvidia.com/v1\"\n",
    "CHAT_API_MODEL = \"meta/llama-3.1-8b-instruct\"\n",
    "# Set Kaggle secret NV_API_KEY in notebook Settings -> Add-ons -> Secrets\n",
    "CHAT_API_KEY = os.environ.get(\"NV_API_KEY\")\n",
    "\n",
    "# Optional: provide known NICE class numbers for p1/p2\n",
    "CLASS_1 = None  # e.g., \"3\" or None\n",
    "CLASS_2 = None # e.g., \"16\" or None\n",
    "\n",
    "# SPSC integration toggles\n",
    "INCLUDE_SPSC = True\n",
    "SPSC_TOP_K = 2\n",
    "\n",
    "from product_similarity.pipeline import run_similarity\n",
    "from eval import evaluate_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T14:16:09.749711Z",
     "iopub.status.busy": "2025-10-15T14:16:09.749477Z",
     "iopub.status.idle": "2025-10-15T14:16:09.756879Z",
     "shell.execute_reply": "2025-10-15T14:16:09.755824Z",
     "shell.execute_reply.started": "2025-10-15T14:16:09.749693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Single-run example (Analyzer + Agents + Judge via evaluate_dataset on 1 row)\n",
    "p1 = \"chemical products used in the manufacture of plastics and in the photocopying industry\"\n",
    "p2 = \"chemical additives for detergents\"\n",
    "\n",
    "# Build a tiny in-memory CSV-like evaluation\n",
    "import pandas as _pd\n",
    "_tmp_csv = \"/kaggle/working/_single_case.csv\"\n",
    "_pd.DataFrame([\n",
    "    {\"Item 1\": p1, \"Item 2\": p2, \"Level of similarity\": 4}\n",
    "]).to_csv(_tmp_csv, index=False)\n",
    "\n",
    "out = evaluate_dataset(\n",
    "    _tmp_csv,\n",
    "    model_name=(USE_ANALYZER_MODEL or None),\n",
    "    agent_model=AGENT_MODEL_ID,\n",
    "    chat_api_base_url=(CHAT_API_BASE_URL if USE_CHAT_API else None),\n",
    "    chat_api_key=(CHAT_API_KEY if USE_CHAT_API else None),\n",
    "    chat_api_model=(CHAT_API_MODEL if USE_CHAT_API else None),\n",
    "    device=DEVICE,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    include_spsc=INCLUDE_SPSC,\n",
    "    spsc_top_k=SPSC_TOP_K,\n",
    ")\n",
    "\n",
    "print(\"Metrics:\", out[\"metrics\"])\n",
    "print(\"Pred overall:\", out[\"results\"][0][\"pred_overall\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T14:16:09.758507Z",
     "iopub.status.busy": "2025-10-15T14:16:09.757712Z",
     "iopub.status.idle": "2025-10-15T14:16:09.768706Z",
     "shell.execute_reply": "2025-10-15T14:16:09.767844Z",
     "shell.execute_reply.started": "2025-10-15T14:16:09.758490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(\"\\nPrompt preview:\\n\", res[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-15T14:16:09.769697Z",
     "iopub.status.busy": "2025-10-15T14:16:09.769505Z",
     "iopub.status.idle": "2025-10-15T14:16:10.107182Z",
     "shell.execute_reply": "2025-10-15T14:16:10.106257Z",
     "shell.execute_reply.started": "2025-10-15T14:16:09.769681Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Batch evaluation from CSV (optional) using multi-agent judge\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json as _json\n",
    "\n",
    "# Prefer 75_samples if available, else fall back to 100_samples\n",
    "csv_75 = code_root / \"data\" / \"75_samples.csv\"\n",
    "csv_100 = code_root / \"data\" / \"100_samples.csv\"\n",
    "csv_path = csv_75 if csv_75.exists() else csv_100\n",
    "\n",
    "if not csv_path.exists():\n",
    "    print(\"CSV not found:\", csv_path)\n",
    "    data_prepared = False\n",
    "else:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    label_col = next((c for c in df.columns if c.strip().lower() == \"level of similarity\"), None)\n",
    "    print(\"Loaded\", len(df), \"rows from:\", csv_path)\n",
    "    data_prepared = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-10-15T14:17:57.343Z",
     "iopub.execute_input": "2025-10-15T14:16:20.769875Z",
     "iopub.status.busy": "2025-10-15T14:16:20.769370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if data_prepared:\n",
    "    # Leverage evaluate_dataset for the whole CSV\n",
    "    out = evaluate_dataset(\n",
    "        str(csv_path),\n",
    "        model_name=(USE_ANALYZER_MODEL or None),\n",
    "        agent_model=AGENT_MODEL_ID,\n",
    "        chat_api_base_url=(CHAT_API_BASE_URL if USE_CHAT_API else None),\n",
    "        chat_api_key=(CHAT_API_KEY if USE_CHAT_API else None),\n",
    "        chat_api_model=(CHAT_API_MODEL if USE_CHAT_API else None),\n",
    "        device=DEVICE,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        include_spsc=INCLUDE_SPSC,\n",
    "        spsc_top_k=SPSC_TOP_K,\n",
    "    )\n",
    "\n",
    "    # Flatten results for convenience\n",
    "    out_rows = []\n",
    "    for r in out[\"results\"]:\n",
    "        out_rows.append({\n",
    "            \"p1\": r[\"product_1\"],\n",
    "            \"p2\": r[\"product_2\"],\n",
    "            \"pred\": r[\"pred_overall\"],\n",
    "            \"label\": r.get(\"gold_overall\"),\n",
    "        })\n",
    "\n",
    "    out_df = pd.DataFrame(out_rows)\n",
    "\n",
    "    # Metrics\n",
    "    metrics = out[\"metrics\"]\n",
    "\n",
    "    # Save outputs\n",
    "    out_path = Path(\"/kaggle/working/batch_results.csv\")\n",
    "    out_df.to_csv(out_path, index=False)\n",
    "\n",
    "    metrics_path = Path(\"/kaggle/working/metrics.json\")\n",
    "    metrics_path.write_text(_json.dumps(metrics, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"Saved:\", out_path)\n",
    "    print(\"Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact 75-samples run: print p1/p2 + class1/class2, then compute predicted score\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from product_similarity.retriever import retrieve_contexts, contexts_from_class_numbers\n",
    "from product_similarity.spsc import retrieve_spsc_contexts\n",
    "from product_similarity.agents import FactorAgent, FactorAgentConfig, evaluate_multiple_factors\n",
    "from product_similarity.judge import LLMJudge, JudgeConfig\n",
    "\n",
    "csv_path = code_root / \"data\" / \"75_samples.csv\"\n",
    "assert csv_path.exists(), f\"CSV not found: {csv_path}\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "N = 10  # adjust how many rows to preview\n",
    "\n",
    "print(f\"Loaded {len(df)} rows from {csv_path}\")\n",
    "\n",
    "# Build agent once\n",
    "agent = FactorAgent(\n",
    "    default=FactorAgentConfig(model_name=AGENT_MODEL_ID, device=DEVICE, max_new_tokens=MAX_NEW_TOKENS),\n",
    "    per_factor=None,\n",
    "    use_chat_api=USE_CHAT_API,\n",
    "    chat_api_base_url=(CHAT_API_BASE_URL if USE_CHAT_API else None),\n",
    "    chat_api_key=(CHAT_API_KEY if USE_CHAT_API else None),\n",
    "    chat_api_model=(CHAT_API_MODEL if USE_CHAT_API else None),\n",
    ")\n",
    "judge = LLMJudge(JudgeConfig(weights={\"Nature\": 0.5, \"Intended Purpose\": 0.5, \"Channel of trade\": 0.0}))\n",
    "factors = [\"Nature\", \"Intended Purpose\", \"Channel of trade\"]\n",
    "\n",
    "for i, r in df.head(N).iterrows():\n",
    "    p1 = str(r.get(\"Item 1\", \"\")).strip()\n",
    "    p2 = str(r.get(\"Item 2\", \"\")).strip()\n",
    "    class1 = r.get(\"class1\")\n",
    "    class2 = r.get(\"class2\")\n",
    "\n",
    "    # Build contexts (prefer class numbers if available), then NICE + optional SPSC\n",
    "    contexts = []\n",
    "    if (class1 is not None) or (class2 is not None):\n",
    "        contexts.extend(contexts_from_class_numbers([class1, class2]))\n",
    "    contexts.extend(retrieve_contexts(p1, p2, top_k=3))\n",
    "    if INCLUDE_SPSC:\n",
    "        contexts.extend(retrieve_spsc_contexts(p1, p2, top_k=SPSC_TOP_K))\n",
    "\n",
    "    shared_ctx = \"\\n\\n\".join(contexts)\n",
    "    per_factor_ctx = {f: shared_ctx for f in factors}\n",
    "\n",
    "    # Run agents + judge\n",
    "    factor_outputs = evaluate_multiple_factors(agent, p1, p2, factors, per_factor_ctx)\n",
    "    judged = judge.combine_factor_scores(factor_outputs)\n",
    "    pred = int(judged.get(\"overall_similarity\", 0))\n",
    "\n",
    "    print(f\"[{i}] p1={p1} | p2={p2} | class1={class1} | class2={class2} -> pred={pred}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8498060,
     "sourceId": 13394037,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8498128,
     "sourceId": 13394411,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
